{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER>\n",
    "</br>\n",
    "<p><font size=\"5\"> Nathan Sanglier </span></p>\n",
    "<p><font size=\"4\">  Global Optimization </font></p>\n",
    "<p></p>\n",
    "<p><font size=\"5\"> Metaheuristics for Constrained Optimization </font></p>\n",
    "</p></br>\n",
    "</p>\n",
    "</CENTER>\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>  <span style=\"color:#FF0000\"> NOT FINISHED YET </span> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style=\"color:#00B8DE\"> 0 - Introduction </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The goal of this notebook is to test 3 very popular techniques in global optimization on a non-convex function with variables' domains constraints: Particle Swarm Optimization, Differential Evolution, and Simulated Annealing. If you want to see the plots and LaTeX equations appear well, please download and run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using LinearAlgebra: norm\n",
    "using Statistics\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#00B8DE\"> I - Particle Swarm Optimization (PSO)</span><a name=\"PSO\"></a>\n",
    "    \n",
    "[Table of contents](#table-of-contents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We consider the function $f(x,y)=x^2+y^2+15(\\cos(x-1)+\\sin(y+1))$, with $(x,y)\\in\\mathcal{E}=[-10,10]\\times [-10,10]$.\n",
    ">- We will implement particle swarm optimization with $n=10$ particles. Initial positions are randomly selected in $\\mathcal{E}$. Speed equation and speed updates are given by\n",
    "\\begin{equation}\n",
    "\\begin{array}{ll}\n",
    "v_{i,t+1} &= \\alpha v_{i,t}+\\beta y_{i,t}(x^{opt}_{i,t}-x_{i,t})+\\gamma z_{i,t}(x^{opt}_{t}-x_{i,t})\\\\\n",
    "x_{i,t+1} &= \\Pi_{\\mathcal{E}}(x_{i,t}+v_{i,t+1})\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "where $\\alpha,\\beta,\\gamma$ are constants, $x^{opt}_{i,t}$ and $x^{opt}_{t}$ are the best solutions obtained so far for particle $i$ and the whole set of particles and   $y_{i,t},z_{i,t}\\sim\\mathcal{U}_{[0,1]}$ introduce randomization of move lengths.\n",
    "$\\Pi_{\\mathcal{E}}$ is the projection on the domain of constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let us plot the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x,y) = x^2 + y^2 + 15 * (cos(x-1) + sin(y+1))\n",
    "\n",
    "# u = [x, y]\n",
    "f(u) = u[1]^2 + u[2]^2 + 15 * (cos(u[1]-1) + sin(u[2]+1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_f = [-1.8872281420673982, -2.2641283236123844];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Min f(x, y) = $(f(opt_f))\")\n",
    "println(\"Optimum at (x, y)* = ($(opt_f[1]), $(opt_f[2]))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "dx_f, dy_f = range(-L, L, 201), range(-L, L, 201)\n",
    "dz_f = f.(dx_f', dy_f)\n",
    "\n",
    "contourf(dx_f, dy_f, dz_f, nlevels = 40, c=:dense, size=(500, 350))\n",
    "scatter!([opt_f[1]], [opt_f[2]], label=\"opt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let us implement particle swarm optimization with $n=10$ particles. Initial positions are randomly selected in $\\mathcal{E}$. Speed equation and speed updates are given by\n",
    "\\begin{equation}\n",
    "\\begin{array}{ll}\n",
    "v_{i,t+1} &= \\alpha v_{i,t}+\\beta y_{i,t}(x^{opt}_{i,t}-x_{i,t})+\\gamma z_{i,t}(x^{opt}_{t}-x_{i,t})\\\\\n",
    "x_{i,t+1} &= \\Pi_{\\mathcal{E}}(x_{i,t}+v_{i,t+1})\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "where $\\alpha,\\beta,\\gamma$ are constants, $x^{opt}_{i,t}$ and $x^{opt}_{t}$ are the best solutions obtained so far for particle $i$ and the whole set of particles and   $y_{i,t},z_{i,t}\\sim\\mathcal{U}_{[0,1]}$ introduce randomization of move lengths.\n",
    "$\\Pi_{\\mathcal{E}}$ is the projection on the domain of constraints.\n",
    "Let us select for instance $\\alpha,\\beta,\\gamma=.9,.5,.5$ and possibly test other choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function find_best_particle_init(p_min)     # Utility function to find the best particle (i.e. the one for which the objective value is the lowest)\n",
    "    \n",
    "    f_values = zeros(n_part)\n",
    "    for i in 1:n_part\n",
    "        f_values[i] = f(p_min[:, i])\n",
    "    end\n",
    "\n",
    "    ind_best = argmin(f_values)\n",
    "    best_particle = p_min[:, ind_best]\n",
    "    return best_particle\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function project_to_domain(particle, L)     # Utility function to project a particle position on the domain\n",
    "    \n",
    "    return max.(min.(particle, L), -L)      # if one of its corrdinates is beyond L, it is set to L and if it is beyond -L, it is set to -L\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particle Swarm Optimization Algorithm\n",
    "function particle_swarm_optim(f, init_pos, init_v, n_iter, L, α, β, γ)\n",
    "\n",
    "    n_dims = size(init_pos)[1]\n",
    "    n_part = size(init_pos)[2]\n",
    "\n",
    "    pos = zeros(n_dims, n_part, n_iter)                 # Note we will store all the positions for each iteration\n",
    "    pos[:, :, 1] = init_pos\n",
    "    \n",
    "    v = init_v                                          # Speed of particles\n",
    "\n",
    "    p_min = pos[:, :, 1]                                # Obviously, at initialization, the best position obtained so far for each particle is the one at initialization\n",
    "    path_p_min = zeros(n_dims, n_part, n_iter)          # We store the path of best position for each particle for each iteration\n",
    "    path_p_min[:, :, 1] = p_min\n",
    "    \n",
    "    p_min_glob = find_best_particle_init(p_min)         # The best position obtained at initialization accross all particles\n",
    "    path_p_min_glob = zeros(n_dims, n_iter)             # We store the path of best position accross all particles for each iteration\n",
    "    path_p_min_glob[:, 1] = p_min_glob\n",
    "\n",
    "    t = 1\n",
    "\n",
    "    while t < n_iter\n",
    "        \n",
    "        r_p = repeat(rand(n_part), 1, 2)'   # Introducing Randomization for the β part\n",
    "        r_g = repeat(rand(n_part), 1, 2)'   # Introducing Randomization for the γ part\n",
    "\n",
    "        v = α * v + β *  r_p .* (p_min - pos[:, :, t]) + γ * r_g .* (repeat(p_min_glob, 1, n_part) - pos[:, :, t])     # Update of speed coordinate for each particle\n",
    "\n",
    "        for i in 1:n_part\n",
    "            pos[:, i, t+1] = project_to_domain(pos[:, i, t] + v[:, i], L)   # Update of position coordinate for each particle\n",
    "            \n",
    "            if f(pos[:, i, t+1]) < f(p_min[:, i])       # If the new position calculated yields to a lower value of f than the best one for this particle so far\n",
    "                \n",
    "                p_min[:, i] = pos[:, i, t+1]            # Then the new position is now the best one for this particle\n",
    "\n",
    "                if f(p_min[:, i]) < f(p_min_glob)       # If this new position is the best one for this particle and is better than the best position accross all particules\n",
    "                    \n",
    "                    p_min_glob = p_min[:, i]            # Then this new position is now the best one accross all particles\n",
    "                end\n",
    "            end\n",
    "\n",
    "            path_p_min[:, i, t+1] = p_min[:, i]\n",
    "        end\n",
    "        \n",
    "        path_p_min_glob[:, t+1] = p_min_glob\n",
    "\n",
    "        t = t + 1\n",
    "    end\n",
    "\n",
    "    return pos, path_p_min, path_p_min_glob\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "α, β, γ = 0.9, 0.5, 0.5     # Paramaeters of the algorithm\n",
    "\n",
    "n_dims = 2      # number of dimensions of the input vector of our function to minimize\n",
    "n_part = 10     # number of particles\n",
    "n_iter = 200    # number of iterations\n",
    "\n",
    "init_pos = L * (2 * rand(n_dims, n_part) .- 1)      # at initialization, each position coordinate of each particle is a realization of a uniform law between -10 and 10\n",
    "init_v = L * (2 * rand(n_dims, n_part) .- 1);       # at initialization, each speed coordinate of each particle is a realization of a uniform law between -10 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, path_p_min, path_p_min_glob = particle_swarm_optim(f, init_pos, init_v, n_iter, L, α, β, γ);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let us plot the trajectory of a few particles, plot the evolution of the coordinates of $x^{opt}_{t}$, plot the particles cloud at some instants, and plot the evolution of the standard deviation and RMSE of the particles cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = contourf(dx_f, dy_f, dz_f, nlevels = 40, c=:dense, size=(800, 600))\n",
    "scatter!([opt_f[1]], [opt_f[2]], label=\"opt\", title=\"Paths of Particles\")\n",
    "for i in 2:4\n",
    "    plot!(pos[1, i, :], pos[2, i, :], marker=:circle, label=\"Path Position Particle $i\")\n",
    "end\n",
    "plot!(path_p_min_glob[1, :], path_p_min_glob[2, :], marker=:square, markersize=4, c=:\"black\", label=\"Path Best Position Overall\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#DAF7A6\">\n",
    "We can see that the few particles plotted converge towards the optimum. At the first iterations, the particles make big movements on the graph that enable diversification, i.e. we tend to converge towards the global minima and not a local minima. This diversification is due to the fact that for each particle, we do not force the particle to keep its best position so far, but we just store the best position accross all particules at each iteration.</br>\n",
    "If we plot the path of the best position accross all particles at each iteration, we can see it converges very fast to the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_graphs = []\n",
    "obs_moments = round.(Int, range(1, n_iter, length=4))\n",
    "\n",
    "for t in obs_moments\n",
    "    graph = contourf(dx_f, dy_f, dz_f, nlevels = 40, c=:dense, size=(200, 150))\n",
    "    scatter!([opt_f[1]], [opt_f[2]], label=\"opt\")\n",
    "    for i in 1:n_part\n",
    "        scatter!([pos[1, i, t]], [pos[2, i, t]], c=:\"green\", label=\"\")\n",
    "    end\n",
    "    scatter!([path_p_min_glob[1, t]], [path_p_min_glob[2, t]], marker=:square, c=:\"black\", label=\"Current Best Position at iter $t\")\n",
    "    push!(L_graphs, graph)\n",
    "end\n",
    "plot(L_graphs..., size=(1000, 750), title=\"Particles Clouds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#DAF7A6\">\n",
    "We can see that as the number of iterations increase the dispersion of particles is more reduced and centered around the optimum. Moreover, the best position accross all particles converges rapidly to the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = zeros(n_part, n_iter)\n",
    "array_std = zeros(n_iter)\n",
    "\n",
    "for t in 1:n_iter\n",
    "    centroid = mean(pos[:, :, t], dims=n_dims)\n",
    "    for i in 1:n_part\n",
    "        dist_matrix[i, t] = norm(pos[:, i, t] - centroid)\n",
    "    end\n",
    "    array_std[t] = std(dist_matrix[:, t])\n",
    "end\n",
    "\n",
    "plot(array_std, label=\"\", title=\"Stdev of particles distances w.r.t. centroid\")\n",
    "xlabel!(\"Nb of iterations\")\n",
    "ylabel!(\"Stdev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#DAF7A6\">\n",
    "As expected, we can see that the cloud of particles is more and more close to its centroïd as the number of iterations increase (i.e. the standard deviation of distance of the particules to the cloud centroid (average position) decreases to 0). The fluctuations at the first iterations are due to the diversification mechanism explained precedently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = zeros(n_part, n_iter)\n",
    "array_rmse = zeros(n_iter)\n",
    "\n",
    "for t in 1:n_iter\n",
    "    for i in 1:n_part\n",
    "        dist_matrix[i, t] = norm(pos[:, i, t] - opt_f)\n",
    "    end\n",
    "    array_rmse[t] = mean(dist_matrix[:, t])\n",
    "end\n",
    "\n",
    "plot(array_rmse, label=\"\", title=\"RMSE of Particles Distance w.r.t. Optimum\")\n",
    "xlabel!(\"Nb of iterations\")\n",
    "ylabel!(\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#DAF7A6\">\n",
    "As expected, we can see that the cloud of particles converges to the optimum, since the RMSE of particles distance to optimum tends to 0. The fluctuations at the first iterations are due to the diversification mechanism explained precedently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#00B8DE\"> II - Differential Evolution (DE)</span><a name=\"DE\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement differential evolution algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function differential_evol_optim(f, init_pos, n_iter, L, CR, F)\n",
    "\n",
    "    n_dims = size(init_pos)[1]\n",
    "    n_part = size(init_pos)[2]\n",
    "\n",
    "    pos = zeros(n_dims, n_part, n_iter)         # Note we will store all the positions for each iteration\n",
    "    pos[:, :, 1] = init_pos\n",
    "    current_pos = init_pos                      # We store the current position for each particle\n",
    "    \n",
    "    p_min_glob = find_best_particle_init(init_pos)          # The best position obtained at initialization accross all particles\n",
    "    path_p_min_glob = zeros(n_dims, n_iter)                 # We store the path of best position accross all particles for each iteration\n",
    "    path_p_min_glob[:, 1] = p_min_glob\n",
    "\n",
    "    t = 1\n",
    "\n",
    "    while t < n_iter\n",
    "\n",
    "        for i in 1:n_part\n",
    "            \n",
    "            neighbors = [k for k in 1:n_part if k != i]         # Particles that can be selected to update the ith particle (i.e. all excluded the ith)\n",
    "            shuffle_inds = randperm(length(neighbors))          \n",
    "            neighbors_selec = neighbors[shuffle_inds[1:3]]      # We pick 3 particles among the possible ones randomly\n",
    "\n",
    "            dim_selec = rand(1:n_dims)      # dimension we select randomly for the crossover\n",
    "\n",
    "            new_pos = zeros(n_dims)         # Potential new position of the particle\n",
    "\n",
    "            for j in 1:n_dims\n",
    "\n",
    "                r = rand()\n",
    "                if r < CR || j == dim_selec     # If crossover rate is above r and the dimension is the one selected above, we do crossover\n",
    "                    \n",
    "                    new_pos[j] = current_pos[j, neighbors_selec[1]] + F * (current_pos[j, neighbors_selec[2]] - current_pos[j, neighbors_selec[3]])\n",
    "                    \n",
    "                else\n",
    "                    new_pos[j] = current_pos[j, i]\n",
    "                end\n",
    "            end\n",
    "\n",
    "            if f(new_pos) < f(current_pos[:, i])            # If the potential new position is better than the current one, we update its current position\n",
    "                current_pos[:, i] = new_pos\n",
    "\n",
    "                if f(current_pos[:, i]) < f(p_min_glob)     # If this new position is better than the best one found so far among all particles\n",
    "                    \n",
    "                    p_min_glob = current_pos[:, i]          # Then we update this best position found so far among all particles\n",
    "                end\n",
    "            end\n",
    "\n",
    "            pos[:, i, t+1] = current_pos[:, i]\n",
    "        end\n",
    "\n",
    "        path_p_min_glob[:, t+1] = p_min_glob\n",
    "\n",
    "        t += + 1\n",
    "    end\n",
    "\n",
    "    return pos, path_p_min_glob\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR, F = 0.9, 0.8 # crossover probability and differential weight\n",
    "\n",
    "n_dims = 2      # number of dimensions of the input vector of our function to minimize\n",
    "n_part = 10     # number of particles\n",
    "n_iter = 50     # number of iterations\n",
    "\n",
    "init_pos = L * (2 * rand(n_dims, n_part) .- 1);      # at initialization, each position coordinate of each particle is a realization of a uniform law between -10 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, path_p_min_glob = differential_evol_optim(f, init_pos, n_iter, L, CR, F);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = contourf(dx_f, dy_f, dz_f, nlevels = 40, c=:dense, size=(800, 600))\n",
    "scatter!([opt_f[1]], [opt_f[2]], label=\"opt\")\n",
    "for i in 3:6\n",
    "    plot!(pos[1, i, :], pos[2, i, :], marker=:circle, label=\"Path Position Particle $i\")\n",
    "end\n",
    "plot!(path_p_min_glob[1, :], path_p_min_glob[2, :], marker=:square, markersize=4, c=:\"black\", label=\"Path Best Position Overall\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#DAF7A6\">\n",
    "We can see here with the differential evolution algorithm, that the few particles plotted converge towards the optimum. At the first iterations, the particles make big movements on the graph that enable diversification with the mechanism of a probability of crossover with an other particle.</br>\n",
    "If we plot the path of the best position accross all particles at each iteration, we can see it converges very fast to the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_graphs = []\n",
    "obs_moments = round.(Int, range(1, n_iter, length=4))\n",
    "\n",
    "for t in obs_moments\n",
    "    graph = contourf(dx_f, dy_f, dz_f, nlevels = 40, c=:dense, size=(200, 150))\n",
    "    scatter!([opt_f[1]], [opt_f[2]], label=\"opt\")\n",
    "    for i in 1:n_part\n",
    "        scatter!([pos[1, i, t]], [pos[2, i, t]], c=:\"green\", label=\"\")\n",
    "    end\n",
    "    scatter!([path_p_min_glob[1, t]], [path_p_min_glob[2, t]], marker=:square, c=:\"black\", label=\"Current Best Position at iter $t\")\n",
    "    push!(L_graphs, graph)\n",
    "end\n",
    "plot(L_graphs..., size=(1000, 750))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#DAF7A6\">\n",
    "We can see that as the number of iterations increase the dispersion of particles is more reduced and centered around the optimum. Moreover, the best position accross all particles converges rapidly to the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = zeros(n_part, n_iter)\n",
    "array_std = zeros(n_iter)\n",
    "\n",
    "for t in 1:n_iter\n",
    "    centroid = mean(pos[:, :, t], dims=n_dims)\n",
    "    for i in 1:n_part\n",
    "        dist_matrix[i, t] = norm(pos[:, i, t] - centroid)\n",
    "    end\n",
    "    array_std[t] = std(dist_matrix[:, t])\n",
    "end\n",
    "\n",
    "plot(array_std, label=\"\", title=\"Stdev of Particles Positions\")\n",
    "xlabel!(\"Nb of iterations\")\n",
    "ylabel!(\"Stdev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#DAF7A6\">\n",
    "As expected, we can see that the cloud of particles is more and more close to its centroïd as the number of iterations increase (i.e. the standard deviation of distance of the particules to the cloud centroid (average position) decreases to 0). The fluctuations at the first iterations are due to the diversification mechanism explained precedently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = zeros(n_part, n_iter)\n",
    "array_rmse = zeros(n_iter)\n",
    "\n",
    "for t in 1:n_iter\n",
    "    for i in 1:n_part\n",
    "        dist_matrix[i, t] = norm(pos[:, i, t] - opt_f)\n",
    "    end\n",
    "    array_rmse[t] = mean(dist_matrix[:, t])\n",
    "end\n",
    "\n",
    "plot(array_rmse, label=\"\", title=\"RMSE of Particles Distance w.r.t. Optimum\")\n",
    "xlabel!(\"Nb of iterations\")\n",
    "ylabel!(\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#DAF7A6\">\n",
    "As expected, we can see that the cloud of particles converges to the optimum, since the RMSE of particles distance to optimum tends to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#00B8DE\"> III - Simulated annealing (SA)</span><a name=\"DE\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function SA(f, x0; σ=1, K=10^3, Ti=10, Tf=1e-2)\n",
    "    #=\n",
    "    - simulated annealing algorithm\n",
    "    - inputs\n",
    "        f: function to minimize\n",
    "        x0: initial position\n",
    "        σ: std of moves \n",
    "        K: number of iterations\n",
    "        Ti: initial temperature\n",
    "        Tf: final temperature\n",
    "    - outputs\n",
    "        points: sequence of tested points\n",
    "    =#\n",
    "    α      = exp(log(Tf/Ti)/K)\n",
    "    tt     = zeros(K) # temperatures\n",
    "    xx     = zeros(K) # positions\n",
    "    yy     = zeros(K) # values of the objective\n",
    "    tt[1]  = Ti       # initial temperature\n",
    "    xx[1]  = x0       # initial position\n",
    "    yy[1]  = f(x0)    # initial value of the objective\n",
    "\n",
    "    for k=2:K\n",
    "        tt[k]  = tt[k-1]*α\n",
    "        x      = xx[k-1] + σ*randn()\n",
    "        y      = f(x)\n",
    "        if (rand() < min(1,exp(-(y-yy[k-1])/tt[k])))\n",
    "            xx[k] = x \n",
    "            yy[k] = y\n",
    "        else \n",
    "            xx[k] = xx[k-1] \n",
    "            yy[k] = yy[k-1]           \n",
    "        end\n",
    "    end\n",
    "\n",
    "    xx,yy\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to optimize\n",
    "f(x)= x^2 - 5cos(2π*x)  #function to be minimized\n",
    "\n",
    "# SA algorithm\n",
    "xx,yy = SA(f, randn()); # points visited by the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "pos = -5:.01:5\n",
    "p1 = plot(pos,f.(pos),label=\"f(x)\")\n",
    "p1 = scatter!(xx,yy,ms=2, label=\"successive positions\",legend=:top) \n",
    "p2 = plot(xx,label=\"successive x values\")\n",
    "plot(p1,p2,layout=(1,2),size=(1000,400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#DAF7A6\">\n",
    "\n",
    "Simulated Annealing is a metaheuristic used to solve optimization problems. Its name comes from annealing in metallurgy, a technique involving heating and controlled cooling of a material to alter its physical properties\n",
    "\n",
    "This notion of slow cooling implemented in the simulated annealing algorithm is interpreted as a slow decrease in the temperature-dependent probability of accepting worse solutions as the solution space is explored. Accepting worse solutions allows for a more extensive search for the global optimal solution, which is important during the first iterations.\n",
    "\n",
    "The decrease of the probability of accepting worse solutions is done by the coefficient $\\alpha$ : at each iteration, we multiply the current temperature by $\\alpha = exp(log(T_f/T_i)/K)$. $T_i$ is the initial temperature and $T_f$ the final temperature. Since K is the number of iterations, we can see that the current temperature at the least iteration is indeed $T_f$.\n",
    "\n",
    "A potential new position is calculated from the current one thanks with a standard deviation given. Then this position is accepted for sure if it leads to a smaller value of the objective function (exp(-(y-yy[k-1])/tt[k]) > 1). But the position can be also accepted even it is worse, with a probability exp(-(y-yy[k-1])/tt[k]). As mentionned above, this probability decreases through time since it depends on the current temperature (which is decreasing at each iteration) at the denominator of the exponential. It also decreases as the solution is bad compared to the current one.\n",
    "\n",
    "\n",
    "On the graphs, we can see that after the diversification phase during the first iterations, the algorithm converges towards the optimum in the next iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let us test the influence of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#DAF7A6\">\n",
    "Let's test the influence of σ on the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "σ_values = [0.1, 1, 10]\n",
    "L_graphs = []\n",
    "\n",
    "for σ_val in σ_values\n",
    "    xx,yy = SA(f, randn(); σ=σ_val, K=10^3, Ti=10, Tf=1e-2)\n",
    "    graph = plot(xx, label=\"σ=$σ_val\")\n",
    "    push!(L_graphs, graph)\n",
    "end\n",
    "\n",
    "plot(L_graphs..., layout=(1,length(σ_values)),size=(1000,400), title=\"Successive x values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#DAF7A6\">\n",
    "\n",
    "We can see that if $\\sigma$ is too small, then we will do too little steps and we will focus only on one region of the graph. This can be dangerous as we could not explore the area where the optimum is. In the opposite, when $\\sigma$ is too high, then we do too big steps and there is too much diversification, it can be dangerous as we could not converge to the optimum because we \"jump over its position\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#DAF7A6\">\n",
    "We won't study the influence of the number of iterations on the algorithm as there is little interest since it is only related to the time we get to converge the the optimum, with a too little number of iterations we could not have the time to converge, wheereas with a too big number, some iterations would be useless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#DAF7A6\">\n",
    "\n",
    "Let's test the influence of $T_i$ (initial temperature) on the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ti_values = [0.1, 10, 100]\n",
    "L_graphs = []\n",
    "\n",
    "for ti_val in Ti_values\n",
    "    xx,yy = SA(f, randn(); σ=1, K=10^3, Ti=ti_val, Tf=1e-2)\n",
    "    graph = plot(xx, label=\"Ti=$ti_val\")\n",
    "    push!(L_graphs, graph)\n",
    "end\n",
    "\n",
    "plot(L_graphs..., layout=(1,length(σ_values)),size=(1000,400), title=\"Successive x values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#DAF7A6\">\n",
    "\n",
    "Here, it is difficult to see well the impact of $T_i$ on the convergence of the algorithm. But we can say that the higher $T_i$ is (compared to a fixed $T_f$), the more brutal will be the changes in the temperature and so the changes in the probability of accepting worse solutions. If $T_i$ is too close to $T_f$, then the convergence can be faster but it could prevent from diversification and we could be stuck in a local optima. If $T_i$ is too far from $T_f$, then the convergence can be slower because of too much diversification in the first iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#DAF7A6\">\n",
    "\n",
    "We don't need to study the impact of $T_f$ (final temperature), as the conclusions will be similar to $T_i$ but in the other way (the closer $T_f$ is from $T_i$, the less brutal the changes in temperature and thus in probability of accpeting worse solution)."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
